Herewith I wanted to collect and display the outcomes of the different models that were trained. Unfortunately all the models lacked reasonable results for most of the generated answers...

All the text-files include a brief description on the background of the model-finetuning, so one might get a feeling for different approaches for the data-preparation or the amount of training-steps.

Basically I for all the runs I fine-tuned the 355M-Model with the GPT-2-Simple library and used a few-shot approach to generate answers on different questions.