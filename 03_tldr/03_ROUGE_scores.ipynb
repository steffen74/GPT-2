{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ROUGE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYwrPLu4AKTX"
      },
      "source": [
        "# Model evaluation\r\n",
        "To evaluate the quality of the generated summarizations the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) score can be used. The ROUGE scoring alogrithm evaluates the similarity between a candidate document (i.e. an abstract) and a reference document (i.e. the summary created by the GPT-2). \r\n",
        "The ROUGE score assesses the quality of a summary by counting how many n-grams in the reference document(s) match the n-grams in the candidate document.\r\n",
        "To calculate ROUGE scores the rouge library is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdpXtkK2mlSJ"
      },
      "source": [
        "## Data preparation\r\n",
        "To calculate ROUGE scores for a set of abstracts and summaries a set of .txt files is needed:\r\n",
        "\r\n",
        "\r\n",
        "*   One .txt file containing the abstracts (one abstract per line).\r\n",
        "*   One .txt file for containing the summaries generated by each of the models (one summary per line; the order of the summaries has to match the order of the abstracts in the abstract documen).\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gafQ4Z7a_Ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fd7d73-d3b2-4096-ca27-49d5cb60bdb0"
      },
      "source": [
        "#Mount gDrive\r\n",
        "from google.colab import drive\r\n",
        "import os\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')  # Mounting GoogleDrive to the content folder\r\n",
        "\r\n",
        "project_dir = 'NLP_scientific-text-generation'\r\n",
        "if not os.path.exists('/content/gdrive/MyDrive/'+project_dir):  # Create a project folder if it does not exist yet\r\n",
        "    os.makedirs('/content/gdrive/MyDrive/'+project_dir)\r\n",
        "os.chdir('/content/gdrive/MyDrive/'+project_dir)  # Changing the working directory to the project folder on GoogleDrive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkCr2bySrmKT"
      },
      "source": [
        "## Score calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0qUsd16AXBX",
        "outputId": "36a2c154-16a8-4aac-8c18-2a877aad254e"
      },
      "source": [
        "#Install the rouge library\r\n",
        "!pip install rouge"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9d3dHuFFaTE"
      },
      "source": [
        "#Calculate scores\r\n",
        "os.chdir('/content/gdrive/My Drive/NLP_scientific-text-generation/')\r\n",
        "from rouge import FilesRouge\r\n",
        "\r\n",
        "files_rouge = FilesRouge()\r\n",
        "rouge_gpt3 = files_rouge.get_scores('abstracts.txt', 'gpt3.txt', avg=True)\r\n",
        "rouge_gpt2_a_os = files_rouge.get_scores('abstracts.txt', 'gpt2_a_os.txt', avg=True)\r\n",
        "rouge_gpt2_a_fs = files_rouge.get_scores('abstracts.txt', 'gpt2_a_fs.txt', avg=True)\r\n",
        "rouge_gpt2_ft_os = files_rouge.get_scores('abstracts.txt', 'gpt2_ft_os.txt', avg=True)\r\n",
        "rouge_gpt2_ft_fs = files_rouge.get_scores('abstracts.txt', 'gpt2_ft_fs.txt', avg=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euu7siOzGaGz"
      },
      "source": [
        "rouge1 = pd.DataFrame(rouge_gpt3)\r\n",
        "index = rouge1.index\r\n",
        "index.name = \"GPT-3\"\r\n",
        "\r\n",
        "rouge2 = pd.DataFrame(rouge_gpt2_a_os)\r\n",
        "index = rouge2.index\r\n",
        "index.name = \"GPT-2 AB OS\"\r\n",
        "\r\n",
        "rouge3 = pd.DataFrame(rouge_gpt2_a_fs)\r\n",
        "index = rouge3.index\r\n",
        "index.name = \"GPT-2 AB FS\"\r\n",
        "\r\n",
        "rouge4 = pd.DataFrame(rouge_gpt2_ft_os)\r\n",
        "index = rouge4.index\r\n",
        "index.name = \"GPT-2 FT OS\"\r\n",
        "\r\n",
        "rouge5 = pd.DataFrame(rouge_gpt2_ft_fs)\r\n",
        "index = rouge5.index\r\n",
        "index.name = \"GPT-2 FT FS\"\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upZW2nIjKKQU",
        "outputId": "37e65346-49cc-4d61-e179-221a9daac4db"
      },
      "source": [
        "#Quick summary\r\n",
        "print(rouge1,'\\n',rouge2,'\\n',rouge3,'\\n',rouge4,'\\n',rouge5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        rouge-1   rouge-2   rouge-l\n",
            "GPT-3                              \n",
            "f      0.238128  0.187547  0.290337\n",
            "p      0.147665  0.117890  0.188683\n",
            "r      0.790589  0.578159  0.764191 \n",
            "               rouge-1   rouge-2   rouge-l\n",
            "GPT-2 AB OS                              \n",
            "f            0.315052  0.135779  0.258778\n",
            "p            0.250132  0.112231  0.202112\n",
            "r            0.445498  0.177827  0.374828 \n",
            "               rouge-1   rouge-2   rouge-l\n",
            "GPT-2 AB FS                              \n",
            "f            0.315807  0.131853  0.255497\n",
            "p            0.249998  0.107336  0.196013\n",
            "r            0.449041  0.177281  0.383152 \n",
            "               rouge-1   rouge-2   rouge-l\n",
            "GPT-2 FT OS                              \n",
            "f            0.254266  0.084814  0.218212\n",
            "p            0.194343  0.066174  0.168405\n",
            "r            0.387685  0.123463  0.340429 \n",
            "               rouge-1   rouge-2   rouge-l\n",
            "GPT-2 FT FS                              \n",
            "f            0.286813  0.125116  0.247005\n",
            "p            0.217478  0.097511  0.193752\n",
            "r            0.447552  0.182467  0.370382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjfV-iWzzsEk"
      },
      "source": [
        "In the columns are the different rouge scores:\r\n",
        "\r\n",
        "\r\n",
        "*   rouge-n : Overlap of n-grams between the abstracts and the summaries\r\n",
        "*   rouge-l: Longest Common Subsequences. Takes sentence level structure similarity naturally into account and identifies longest co-occurring in sequence n-grams automatically\r\n",
        "\r\n",
        "Three different test statistics are reported:\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   precision (p): Proportion of the n-grams in the generated summary that are also present in the abstract (i.e. a rouge-1 precision of 0.21 means that 21% percent of unigrams in the generated summary are also present in the abstract).\r\n",
        "*   recall (r): Proportion of the n-grams in the abstract that are also present in the summary (i.e. a rouge-1 recall of 0.45 means that 45% of the unigrams in the abstract are also present in the summary).\r\n",
        "*   f-score: Measure of robustness and precision. Harmonic mean of your precision and recall. Greatest when precision and recall are equal. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    }
  ]
}