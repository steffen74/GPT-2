{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ROUGE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYwrPLu4AKTX"
      },
      "source": [
        "# Model evaluation\r\n",
        "To evaluate the quality of the generated summarizations the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) score can be used. The ROUGE scoring alogrithm evaluates the similarity between a candidate document (i.e. an abstract) and a reference document (i.e. the summary created by the GPT-2). \r\n",
        "The ROUGE score assesses the quality of a summary by counting how many n-grams in the reference document(s) match the n-grams in the candidate document.\r\n",
        "To calculate ROUGE scores the rouge library is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdpXtkK2mlSJ"
      },
      "source": [
        "## Data preparation\r\n",
        "To calculate ROUGE scores for a set of abstracts and summaries a set of .txt files is needed:\r\n",
        "\r\n",
        "\r\n",
        "*   One .txt file containing the abstracts (one abstract per line).\r\n",
        "*   One .txt file for containing the summaries generated by each of the models (one summary per line; the order of the summaries has to match the order of the abstracts in the abstract documen).\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gafQ4Z7a_Ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17754324-0c7f-40d7-ca5f-f32b3e50e339"
      },
      "source": [
        "#Mount gDrive\r\n",
        "from google.colab import drive\r\n",
        "import os\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')  # Mounting GoogleDrive to the content folder\r\n",
        "\r\n",
        "project_dir = 'NLP_scientific-text-generation'\r\n",
        "if not os.path.exists('/content/gdrive/MyDrive/'+project_dir):  # Create a project folder if it does not exist yet\r\n",
        "    os.makedirs('/content/gdrive/MyDrive/'+project_dir)\r\n",
        "os.chdir('/content/gdrive/MyDrive/'+project_dir)  # Changing the working directory to the project folder on GoogleDrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkCr2bySrmKT"
      },
      "source": [
        "## Score calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0qUsd16AXBX",
        "outputId": "88e25a75-cc48-404f-846a-f86623abae1e"
      },
      "source": [
        "#Install the rouge library\r\n",
        "!pip install rouge"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9d3dHuFFaTE"
      },
      "source": [
        "#Calculate scores\r\n",
        "os.chdir('/content/gdrive/My Drive/NLP_scientific-text-generation/')\r\n",
        "from rouge import FilesRouge\r\n",
        "\r\n",
        "files_rouge = FilesRouge()\r\n",
        "rouge_gpt3_os = files_rouge.get_scores('abstracts.txt', 'gpt3_os.txt', avg=True)\r\n",
        "rouge_gpt3_fs = files_rouge.get_scores('abstracts.txt', 'gpt3_fs.txt', avg=True)\r\n",
        "rouge_gpt2_a_os = files_rouge.get_scores('abstracts.txt', 'gpt2_a_os.txt', avg=True)\r\n",
        "rouge_gpt2_a_fs = files_rouge.get_scores('abstracts.txt', 'gpt2_a_fs.txt', avg=True)\r\n",
        "rouge_gpt2_ft_os = files_rouge.get_scores('abstracts.txt', 'gpt2_ft_os.txt', avg=True)\r\n",
        "rouge_gpt2_ft_fs = files_rouge.get_scores('abstracts.txt', 'gpt2_ft_fs.txt', avg=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euu7siOzGaGz"
      },
      "source": [
        "import pandas as pd\r\n",
        "rouge1 = pd.DataFrame(rouge_gpt3_os)\r\n",
        "index = rouge1.index\r\n",
        "index.name = \"GPT-3 OS\"\r\n",
        "\r\n",
        "rouge2 = pd.DataFrame(rouge_gpt3_fs)\r\n",
        "index = rouge2.index\r\n",
        "index.name = \"GPT-3 FS\"\r\n",
        "\r\n",
        "rouge3 = pd.DataFrame(rouge_gpt2_a_os)\r\n",
        "index = rouge3.index\r\n",
        "index.name = \"GPT-2 AB OS\"\r\n",
        "\r\n",
        "rouge4 = pd.DataFrame(rouge_gpt2_a_fs)\r\n",
        "index = rouge4.index\r\n",
        "index.name = \"GPT-2 AB FS\"\r\n",
        "\r\n",
        "rouge5 = pd.DataFrame(rouge_gpt2_ft_os)\r\n",
        "index = rouge5.index\r\n",
        "index.name = \"GPT-2 FT OS\"\r\n",
        "\r\n",
        "rouge6 = pd.DataFrame(rouge_gpt2_ft_fs)\r\n",
        "index = rouge6.index\r\n",
        "index.name = \"GPT-2 FT FS\"\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upZW2nIjKKQU",
        "outputId": "0d4458a5-0fde-4f9e-e872-150cdf961c09"
      },
      "source": [
        "#Quick summary\r\n",
        "print(rouge1,'\\n',rouge2,'\\n',rouge3,'\\n',rouge4,'\\n',rouge5,'\\n',rouge6)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           rouge-1   rouge-2   rouge-l\n",
            "GPT-3 OS                              \n",
            "f         0.151580  0.097994  0.190932\n",
            "p         0.085264  0.055060  0.112048\n",
            "r         0.755006  0.503604  0.710610 \n",
            "            rouge-1   rouge-2   rouge-l\n",
            "GPT-3 FS                              \n",
            "f         0.142686  0.095379  0.189342\n",
            "p         0.080542  0.053712  0.111353\n",
            "r         0.766042  0.551002  0.735236 \n",
            "               rouge-1   rouge-2   rouge-l\n",
            "GPT-2 AB OS                              \n",
            "f            0.131404  0.050276  0.143804\n",
            "p            0.075910  0.029046  0.086602\n",
            "r            0.554176  0.204743  0.473762 \n",
            "               rouge-1   rouge-2   rouge-l\n",
            "GPT-2 AB FS                              \n",
            "f            0.133295  0.053752  0.148717\n",
            "p            0.078052  0.031571  0.089502\n",
            "r            0.545004  0.216439  0.485919 \n",
            "               rouge-1   rouge-2   rouge-l\n",
            "GPT-2 FT OS                              \n",
            "f            0.102876  0.034575  0.128974\n",
            "p            0.059176  0.019741  0.077252\n",
            "r            0.476540  0.154280  0.443620 \n",
            "               rouge-1   rouge-2   rouge-l\n",
            "GPT-2 FT FS                              \n",
            "f            0.112802  0.049261  0.131157\n",
            "p            0.064373  0.027987  0.077921\n",
            "r            0.535780  0.238214  0.471604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjfV-iWzzsEk"
      },
      "source": [
        "In the columns are the different rouge scores:\r\n",
        "\r\n",
        "\r\n",
        "*   rouge-n : Overlap of n-grams between the abstracts and the summaries\r\n",
        "*   rouge-l: Longest Common Subsequences. Takes sentence level structure similarity naturally into account and identifies longest co-occurring in sequence n-grams automatically\r\n",
        "\r\n",
        "Three different test statistics are reported:\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   precision (p): Proportion of the n-grams in the generated summary that are also present in the abstract (i.e. a rouge-1 precision of 0.21 means that 21% percent of unigrams in the generated summary are also present in the abstract).\r\n",
        "*   recall (r): Proportion of the n-grams in the abstract that are also present in the summary (i.e. a rouge-1 recall of 0.45 means that 45% of the unigrams in the abstract are also present in the summary).\r\n",
        "*   f-score: Measure of robustness and precision. Harmonic mean of your precision and recall. Greatest when precision and recall are equal. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    }
  ]
}